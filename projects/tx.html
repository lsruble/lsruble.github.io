<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Name - robruble</title>
    <link rel="stylesheet" href="../styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Space+Mono&display=swap" rel="stylesheet">
    <style>
        .project-info {
            display: flex;
            justify-content: space-between;
            margin-bottom: 2rem;
        }
        .project-info div {
            flex: 1;
        }
        .project-description img {
            max-width: 100%;
            height: auto;
            margin: 1rem 0;
        }
    </style>
</head>
<body>
    <header>
        <h1>Deep Learning for VR and Empathy</h1>
        <nav style="display: flex; gap: 20px;">
            <a href="../index.html">Home</a>
            <a href="../projects.html">Projects</a>
        </nav>
    </header>
    <main>
        <section class="project-info">
            <div>
                <h2>Duration</h2>
                <p>February 2024 - July 2024</p>
            </div>
            <div>
                <h2>Location</h2>
                <p>Université de Technologie de Compiègne</p>
            </div>
        </section>
        <section class="project-description">
            <h2>Description</h2>
            <p> This project explores the intersection of virtual reality (VR) and emotion recognition, focusing on the challenges posed by facial occlusions when users wear VR headsets. We developed an augmented reality (AR) algorithm that overlays a virtual VR headset onto faces, allowing us to simulate the conditions of VR usage in datasets. Using facial landmarks and real-time transformations, the algorithm accurately positions and scales the headset, creating a realistic simulation.</p>
            <img src="images/TX_1.png" alt="Project Image 2" width="600" height="400">
            <p>We then applied this AR technique to a dataset, training a simple model to recognize emotions from faces partially obscured by a VR headset. The results, however, revealed limitations in emotion detection due to the occlusion of key facial regions, such as the eyes and eyebrows, which are crucial for recognizing certain emotions like anger. Despite achieving a 67% accuracy rate during training, the model struggled with real-world scenarios, reflecting the complexity of emotion recognition in VR.
                To address these challenges, we propose integrating additional data from eye-tracking and head position analysis, which could enhance the model's ability to interpret emotions even when facial features are obscured. This multimodal approach aims to provide a more comprehensive understanding of the user's emotional state in VR environments, paving the way for more immersive and emotionally responsive VR experiences.</p>
        </section>
    </main>
    <footer>
        <p>&copy; 2024 robruble</p>
    </footer>
</body>
</html>
